{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.파이썬을 이용한 웹 크롤링 기본(requests 패키지와 bs4 패키지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- requests 패키지: 웹페이지에 접속해서 접속한 웹페이지를 객체로 가져올 수 있도록 해준다\n",
    "   - get: 사용자가 입력한 주소의 html페이지를 가져온다.\n",
    "- bs4 패키지: html태그를 파싱(해석)해서 필요한 데이타를 추출할 수 있도록 해준다.\n",
    "  - BeautifulSoup : bs4 패키지 내의 클래스로 html 문서를 파싱 가능한 객체로 만들     어 준다.\n",
    "\n",
    "   - find : 찾고 싶은 전체 테그를 찾아준다.\n",
    "   - find_all : 찾고 싶은 전체 테그를 찾아준다.   \n",
    "   - get_text : 해당 태그의 데이타를 반환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 웹페이지는 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- html,css,javascript 등으로 구성된다.\n",
    "- html 언어는 웹페이지를 만드는 기본 언어로 <태그> 보여줄 내용 </태그> 형식으   로 되어 있음\n",
    "- 알고 있어야 할 html 기본 태그\n",
    "    - html,head,body,title,meta,h1-h6,p,br,strong,b,em,i,span,hr등\n",
    "    - ul,ol,li,dl,dt,dd등\n",
    "    - table,thread,tbody,tr,th 등\n",
    "    - img,a,div 등\n",
    "- 웹페이지에 css(글자모양,크기,색깔,정렬 등)을 적용하는 3가지 방법\n",
    "    - html 태그에 일일히 style 속성을 지정하는 방법(inline 방식)   \n",
    "    - head 태그 안에 style 태그를 만들고, 그 안에 style 관련 코드를 쓰는 방식 (내부 style 시트 방식)\n",
    "    - 별도의 파일에 style을 만들어 놓은 후 이용하는 방식(외부 style 시트 방식)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 웹페이지에서 원하는 데이타를 크롤링 하기 위해서는 ?\n",
    "  - requests 패키지와 BeautifulSoup 클래스를 이용하여 크롤링 코드를 작성한다.\n",
    "  - 크롤링할 데이타의 html 태그를 확인한다.\n",
    "  - 크롤링 할 html태그가 유일하면 find('태그')를 호출한다.\n",
    "  - 동일한 html 태그가 여러개 있을 때 그 중 하나를 선택하는 경우에는\n",
    "      - find('태그',class_='스타일이름')\n",
    "      - find('태그','스타일이름')# 위에서 class_를 생략할 수 있다\n",
    "      - find('태그',attrs={'속성':'값','속성:'값'',....})\n",
    "      - find(Id='이름')\n",
    "  - 동일한 html 태그가 여러개 있을 때 모두를 다 선택해야 하는 경우에는\n",
    "      - find_all('태그')함수를 이용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.HTML 태그만을 이용한 크롤링 해보기\n",
    " - 문자열 변수 page에 아래와 같은 html 문서의 내용이 저장되어 있다.\n",
    " - HTML 태그와 속성중 title, h1, p, id, align 을 이용하여 크롤링 해보시오\n",
    " - h1 , p 태그처럼 여러번 반복되어 나와 있는 태그의 경우 모든 데이타를 다크롤링 해보시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>웹 크롤링 기술</title>\n",
      "웹 크롤링 기술\n",
      "[1]파이썬 문법\n",
      "[5] 프로젝트 하기 \n",
      "[5] Selenium 패키지 사용법 익히기\n",
      "2\n",
      "[1]파이썬 문법\n",
      "[5] 프로젝트 하기 \n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "page = \"\"\"\n",
    "\n",
    "<html>\n",
    "      <head>\n",
    "              <meta charset = \"utf-8\">\n",
    "              <title>웹 크롤링 기술</title>\n",
    "      </head>\n",
    "      <body>\n",
    "              <h1 class = 'mystyle'>[1]파이썬 문법</h1>\n",
    "              <h2>[2] html 사용법</h2>\n",
    "              <h3>[3] BeautifulSoap 클래스 사용법</h2>\n",
    "              <p id='body1' align='left'>[4] open API 사용법 익히기</p>\n",
    "              <p id='body2' align='center'>[5] Selenium 패키지 사용법 익히기</p>\n",
    "              <p id='body3' align='right'>[6] 웹크롤러 만드는 법</p>\n",
    "              <h1 id='body4'>[5] 프로젝트 하기 </h1>\n",
    "      </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(page,'html.parser')\n",
    "data_title = soup.find('title')\n",
    "print(data_title)\n",
    "print(data_title.get_text())\n",
    "\n",
    "data_h1 = soup.find('h1')\n",
    "print(data_h1.get_text())\n",
    "\n",
    "\n",
    "data_p = soup.find(id = 'body4')\n",
    "print(data_p.get_text())\n",
    "\n",
    "data_p = soup.find('p',attrs={'align':'center'})\n",
    "print(data_p.get_text())\n",
    "\n",
    "\n",
    "data_h1_all = soup.find_all('h1')\n",
    "print(len(data_h1_all))\n",
    "for i in data_h1_all :\n",
    "    print(i.get_text())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "웹 크롤링 기술\n",
      "[1]파이썬 문법\n",
      "[5] 프로젝트 하기 \n",
      "[5] Selenium 패키지 사용법 익히기\n",
      "2\n",
      "[1]파이썬 문법\n",
      "[5] 프로젝트 하기 \n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "page = \"\"\"\n",
    "\n",
    "<html>\n",
    "      <head>\n",
    "              <meta charset = \"utf-8\">\n",
    "              <title>웹 크롤링 기술</title>\n",
    "      </head>\n",
    "      <body>\n",
    "              <h1 class = 'mystyle'>[1]파이썬 문법</h1>\n",
    "              <h2>[2] html 사용법</h2>\n",
    "              <h3>[3] BeautifulSoap 클래스 사용법</h2>\n",
    "              <p id='body1' align='left'>[4] open API 사용법 익히기</p>\n",
    "              <p id='body2' align='center'>[5] Selenium 패키지 사용법 익히기</p>\n",
    "              <p id='body3' align='right'>[6] 웹크롤러 만드는 법</p>\n",
    "              <h1 id='body4'>[5] 프로젝트 하기 </h1>\n",
    "      </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(page,'html.parser')\n",
    "data_1 = soup.find('title')\n",
    "\n",
    "print(data_1.get_text())\n",
    "\n",
    "data_2 = soup.find('h1',class_='mystyle')\n",
    "print(data_2.get_text())\n",
    "\n",
    "data_3 = soup.find(id = 'body4')\n",
    "print(data_3.get_text())\n",
    "\n",
    "data_4 = soup.find('p',attrs={'align':'center'})\n",
    "print(data_4.get_text())\n",
    "\n",
    "data_5 = soup.find_all('h1')\n",
    "print(len(data_5))\n",
    "for data in data_5:\n",
    "    print(data.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. html코드와 css를 이용한 크롤링 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 학번 \n",
      " 이름 \n",
      "sample test\n",
      "20123123\n",
      "홍길동\n",
      "20123124\n",
      "김철수\n",
      "\n",
      "20123124\n",
      "\n",
      "20123124\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "page = \"\"\"\n",
    "\n",
    "<html>\n",
    "  <head>\n",
    "        <title>sample test</title>\n",
    "        <meta charset=\"utf-8\">\n",
    "<!-- 내부스타일 시트  \n",
    "        <style type=\"text/css\">\n",
    "              td{\n",
    "                   font-size: 2em;\n",
    "                   font-family:Gothic;\n",
    "                   text-align=cener;\n",
    "                   color:blue;\n",
    "                 }\n",
    "        </style>\n",
    "\n",
    "-->\n",
    "<!-- 외부스타일 (외부 파일에 스타일을 정의한후 href 속성에 파일 이름을 지정한다.)-->\n",
    "      <link rel=\"stylesheet\" type=\"text/css\" href=\"style.css\">\n",
    "  </head>\n",
    "  <body>\n",
    "      <table border=1 width=\"500\">\n",
    "             <thead>\n",
    "                   <tr>\n",
    "                           <th> 학번 </th>\n",
    "                           <th> 이름 </th>\n",
    "                   </tr>\n",
    "             </thead>\n",
    "              <tbody>\n",
    "                           <tr>\n",
    "                                 <td style='text-align:right;color:red;font-size:16px'>20123123</td>\n",
    "                                  <td class='normal'>홍길동</td>\n",
    "                           </tr>\n",
    "                           <tr>\n",
    "                                 <td class='highlight'>20123124</td>\n",
    "                                 <td class='normal'>김철수</td> \n",
    "                           <tr>\n",
    "               </tbody>\n",
    "      </table>\n",
    "   </body>\n",
    "\n",
    "\"\"\"\n",
    "soup = BeautifulSoup(page,'html.parser')\n",
    "data_th = soup.find_all('th')\n",
    "for i in data_th:\n",
    "    print(i.get_text())\n",
    "data_title = soup.find('title')\n",
    "print(data_title.get_text())\n",
    "\n",
    "data_td = soup.find_all('td')\n",
    "for i in data_td:\n",
    "    print(i.get_text())\n",
    "\n",
    "print()\n",
    "data_td = soup.find('td',class_='highlight')\n",
    "print(data_td.get_text())\n",
    "\n",
    "print()\n",
    "data_td = soup.find('td','highlight')\n",
    "print(data_td.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 학번 \n",
      " 이름 \n",
      "sample test\n",
      "20123123\n",
      "홍길동\n",
      "20123124\n",
      "김철수\n",
      "20123124\n",
      "20123124\n"
     ]
    }
   ],
   "source": [
    "page = \"\"\"\n",
    "\n",
    "<html>\n",
    "  <head>\n",
    "        <title>sample test</title>\n",
    "        <meta charset=\"utf-8\">\n",
    "<!-- 내부스타일 시트  \n",
    "        <style type=\"text/css\">\n",
    "              td{\n",
    "                   font-size: 2em;\n",
    "                   font-family:Gothic;\n",
    "                   text-align=cener;\n",
    "                   color:blue;\n",
    "                 }\n",
    "        </style>\n",
    "\n",
    "-->\n",
    "<!-- 외부스타일 (외부 파일에 스타일을 정의한후 href 속성에 파일 이름을 지정한다.)-->\n",
    "      <link rel=\"stylesheet\" type=\"text/css\" href=\"style.css\">\n",
    "  </head>\n",
    "  <body>\n",
    "      <table border=1 width=\"500\">\n",
    "             <thead>\n",
    "                   <tr>\n",
    "                           <th> 학번 </th>\n",
    "                           <th> 이름 </th>\n",
    "                   </tr>\n",
    "             </thead>\n",
    "              <tbody>\n",
    "                           <tr>\n",
    "                                 <td style='text-align:right;color:red;font-size:16px'>20123123</td>\n",
    "                                  <td class='normal'>홍길동</td>\n",
    "                           </tr>\n",
    "                           <tr>\n",
    "                                 <td class='highlight'>20123124</td>\n",
    "                                 <td class='normal'>김철수</td> \n",
    "                           <tr>\n",
    "               </tbody>\n",
    "      </table>\n",
    "   </body>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(page,'html.parser')\n",
    "\n",
    "data_1 = soup.find_all('th')\n",
    "for data in data_1:\n",
    "    print(data.get_text())\n",
    "    \n",
    "\n",
    "data_2 = soup.find_all('title')\n",
    "\n",
    "for data in data_2:\n",
    "    print(data.get_text())\n",
    "    \n",
    "data_3 = soup.find_all('td')\n",
    "\n",
    "for data in data_3:\n",
    "    print(data.get_text())\n",
    "    \n",
    "    \n",
    "data_4 = soup.find_all('td',class_ = 'highlight')\n",
    "\n",
    "\n",
    "print(data_4[0].get_text())\n",
    "    \n",
    "    \n",
    "data_5 = soup.find_all('td','highlight')\n",
    "\n",
    "print(data_5[0].get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 웹페이지에서 원하는 부분 크롤링 해보기\n",
    " - 크롬 브라우저를 이용하여 원하는 페이지를 검색한다\n",
    " - 마우스 오른쪽 버튼을 눌러 페이지 소스 보기를 선택한다.\n",
    " - 크롤링할 데이타의 html 태그를 확인한다.\n",
    " - requests 패키지와 BeautifulSoup클래스를 이용하여 크롤링 코드를 작성한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1웹 페이지 크롤링 해보기 1(네이버 사전 페이지의 제목 크롤링 해보기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네이버 사전 (NAVER dictionary)\n",
      "<title>네이버 사전 (NAVER dictionary)</title>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "page = requests.get('http://dict.naver.com/')\n",
    "soup = BeautifulSoup(page.content,'html.parser')\n",
    "\n",
    "data_title = soup.find('title')\n",
    "print(data_title.get_text())\n",
    "print(data_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네이버 사전 (NAVER dictionary)\n",
      "<title>네이버 사전 (NAVER dictionary)</title>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "page = requests.get('https://dict.naver.com/')\n",
    "soup = BeautifulSoup(page.content,'html.parser')\n",
    "\n",
    "data_title = soup.find('title')\n",
    "\n",
    "print(data_title.get_text())\n",
    "print(data_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 웹 페이지 크롤링 해보기 2(다음 홈페이지의 IT뉴스 헤드라인 크롤링 해보기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<strong class=\"tit_thumb\">\n",
      "<a class=\"link_txt\" data-tiara-custom=\"contentUniqueKey=hamny-20200617155341313\" data-tiara-id=\"20200617155341313\" data-tiara-layer=\"article_main\" data-tiara-ordnum=\"1\" data-tiara-type=\"harmony\" href=\"https://news.v.daum.net/v/20200617155341313\">美 화웨이 제재..반도체 70억 달러 손실 '역효과'</a>\n",
      "</strong>\n",
      "\n",
      "\n",
      "美 화웨이 제재..반도체 70억 달러 손실 '역효과'\n",
      "\n",
      "<bound method Tag.get_text of <p>넥슨코리아가 자회사 네오플로부터 1조원 규모의 자금을 또 빌렸다.</p>>\n",
      "\n",
      "넥슨코리아가 자회사 네오플로부터 1조원 규모의 자금을 또 빌렸다.\n",
      "넥슨코리아는 27일 자회사 네오플로부터 1조1140억 원을 차입했다고 공시했다.\n",
      "업계는 넥슨코리아가 지난해 네오플로부터 차입한 자금을 M&A 등에 써온 만큼 이번에도 대규모 M&A를 염두에 둔 행보로 본다.\n",
      "넥슨코리아는 지난해 9월 네오플로부터 4000억원을 차입했을 당시 3500억원을 이커머스 업체 위메프의 모회사 원더홀딩스에 투자해 지분 11.1%를 확보한 바 있다.\n",
      "Translated by kakao i\n",
      "\n",
      "넥슨코리아가 자회사 네오플로부터 1조원 규모의 자금을 또 빌렸다. 지난 8일 3820억원을 빌린데 이어 이달 들어서만 두 번째다. 이로써 넥슨코리아가 확보한 현금성 자산은 약 2조원이 넘을 전망이다. 넥슨코리아는 자금 차입 목적을 명시하진 않았지만, 업계는 대규모 인수·합병(M&A) 가능성에 무게를 싣고 있다.\n",
      "넥슨코리아는 27일 자회사 네오플로부터 1조1140억 원을 차입했다고 공시했다. 이자율은 4.6%며 차입기간은 내년 4월까지다. 넥슨코리아는 이번 차입금을 운영자금 및 투자재원으로 활용한다는 입장이다.\n",
      "업계는 넥슨코리아가 지난해 네오플로부터 차입한 자금을 M&A 등에 써온 만큼 이번에도 대규모 M&A를 염두에 둔 행보로 본다. 넥슨코리아는 지난해 9월 네오플로부터 4000억원을 차입했을 당시 3500억원을 이커머스 업체 위메프의 모회사 원더홀딩스에 투자해 지분 11.1%를 확보한 바 있다.\n",
      "업계 한 관계자는 \"넥슨이 지난해 하반기 조직 개편을 단행했고 여러 게임 프로젝트를 정리하는 등 내부 정비를 마친 상태\"라며 \"넥슨이 향후 기업 인수를 통해 몸집 키우기에 적극 나설 것으로 보인다\"고 말했다.\n",
      "네오플은 넥슨코리아의 핵심 계열사다. 중국 시장에서 인기를 끌고 있는 PC온라인게임 ‘던전앤파이터’를 개발했다. 올해 신작 ‘던전앤파이터 모바일’을 중국 시장에 선보일 예정이다.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "page= requests.get('https://news.daum.net/digital')\n",
    "soup = BeautifulSoup(page.content,'html.parser')\n",
    "\n",
    "data = soup.find('strong',class_='tit_thumb')\n",
    "print(data)\n",
    "print()\n",
    "print(data.get_text())\n",
    "\n",
    "\n",
    "page2= requests.get('https://news.v.daum.net/v/20200427194517096')\n",
    "soup = BeautifulSoup(page2.content,'html.parser')\n",
    "data_p=soup.find('p')\n",
    "print(data_p.get_text)\n",
    "\n",
    "print()\n",
    "\n",
    "data_p = soup.find_all('p')\n",
    "\n",
    "for i in data_p:\n",
    "    print(i.get_text())\n",
    "    #print(i.text)\n",
    "    #print(i.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "美 화웨이 제재..반도체 70억 달러 손실 '역효과'\n",
      "도널드 트럼프 미국 대통령의 일방적인 화웨이 제재가 결국 미국 반도체 업계에 심각한 피해를 줄 뿐이라는 조사결과가 나왔다.\n",
      "미국 시장조사업체 스트래티지 애널리틱스(SA)는 최근 '화웨이 제재: 통신, 글로벌 반도체 및 미국경제에 미칠 악영향' 제목의 보고서에서 미국 반도체 업계가 화웨이 제재로 인해 약 70억 달러의 사업 손실을 입을 것이라고 분석했다.\n",
      "Translated by kakao i\n",
      "(지디넷코리아=김태진 기자)도널드 트럼프 미국 대통령의 일방적인 화웨이 제재가 결국 미국 반도체 업계에 심각한 피해를 줄 뿐이라는 조사결과가 나왔다.\n",
      "미국 시장조사업체 스트래티지 애널리틱스(SA)는 최근 '화웨이 제재: 통신, 글로벌 반도체 및 미국경제에 미칠 악영향' 제목의 보고서에서 미국 반도체 업계가 화웨이 제재로 인해 약 70억 달러의 사업 손실을 입을 것이라고 분석했다.\n",
      "SA는 브로드컴의 연 매출에서 화웨이가 차지하는 비중이 약 8.7%(20억 달러)이며, 인텔은 최소 15억 달러의 데이터센터 칩을 매년 화웨이에 판매하는 것으로 추산했다.\n",
      "\n",
      "화웨이가 전 세계 반도체 시장에서 차지하는 비중도 크다. 화웨이는 매년 200억 달러 이상의 반도체를 구매하는 데 이는 전체의 약 5%(4천억원)에 이른다. 화웨이의 구매 감소는 곧 미국을 포함한 반도체 기업들의 매출 하락으로 이어졌다. 보스턴컨설팅그룹은 최근 미-중 무역전쟁 확대로 세계 반도체 수요가 약 40% 줄어들 것으로 분석한 바 있다.\n",
      "화웨이 제재에 따른 나비효과는 5G 시장에도 타격을 줄 전망이다. SA는 전 세계 5G 표준을 정립하는 3GPP의 핵심 회원인 화웨이가 장비를 제공할 수 없으면 5G 인프라를 구축해야 되는 통신사들이 계획에 차질을 입을 수밖에 없다고 지적했다.\n",
      "미국 기술조사업체 그레이비서비스와 데이터조사업체 앰플리파이드가 최근 5G 관련 표준기술특허(SEP)에 관해 공동 진행한 결과 화웨이가 302건(19%)으로 가장 많은 SEP를 보유한 것으로 나타났다. SEP란 특정 사업에 채택된 표준기술을 구현하는 데 반드시 필요한 기술 특허다. 결국, 미국이 글로벌 5G 공급망에서 화웨이를 배제하려고 해도 화웨이에 특허료를 지불할 수밖에 없는 상황이다.\n",
      "미 상무부가 15일(현지시간) 화웨이와 거래 전면금지 규정을 수정한다며 ‘화웨이 봉쇄령’을 일부 해제한 것도 이 같은 요인이 작용했을 것이란 분석이다.\n",
      "미국 기업들이 화웨이 제재에 대해 정부와 대립각을 세우고 있는 것도 같은 이유다. 최근 몇 달 동안 록히드 마틴, 아마존, 애플, 3M, 포드자동차 등의 기업을 대표하는 무역 단체는 미국의 광범위한 규정을 수정하라는 요구를 트럼프 정부에 제기했다.\n",
      "미국 법률가들은 만약 미 정부의 제재 규정이 집행된다면 그 어떤 곳도 라우터, 스위치, 인터넷 서비스, 클라우드 네트워크 등을 자유롭게 이용할 수 없게 된다고 설명했다.\n",
      "미국의 대형 로펌 중 한 곳인 코빙턴앤벌링의 사만다 클라크 변호사는 \"화웨이 시스템은 중국과 유럽, 아프리카 일대에서 매우 보편적으로 사용되고 있다\"며 \"일부 기업들은 다양한 경로를 통해 미국 정부에 제품을 판매하고 있지만 실제로는 자신이 미 정부의 조달망에 얼마나 관여돼 있는지 알 지 못한다\"고 말했다.\n",
      "기업이 사용하는 여러 부품 중 일부 구성이 화웨이 장비이더라도 이를 쉽게 알기 어려운 것이 현실이란 것이다.\n",
      "김태진 기자(tjk@zdnet.co.kr)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "page = requests.get('https://news.v.daum.net/v/20200617155341313')\n",
    "soup = BeautifulSoup(page.content,'html.parser')\n",
    "\n",
    "data_headLine = soup.find('h3',class_='tit_view')\n",
    "print(data_headLine.get_text())\n",
    "\n",
    "data_content = soup.find_all('p')\n",
    "for data in data_content:\n",
    "    print(data.get_text())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
